{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3ca1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035a83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        head_dim: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.query = nn.Linear(embedding_dim, head_dim)\n",
    "        self.key = nn.Linear(embedding_dim, head_dim)\n",
    "        self.value = nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear\n",
    "        # (B, T, E) -> (B, T, H)\n",
    "        q = self.query(x)\n",
    "        # (B, T, E) -> (B, T, H)\n",
    "        k = self.key(x)\n",
    "        # (B, T, E) -> (B, T, H)\n",
    "        v = self.value(x)\n",
    "\n",
    "        _, T, H = k.shape\n",
    "\n",
    "        # MatMul (Query and Transpose of Key)\n",
    "        # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "        qk = q @ k.transpose(-1, -2)\n",
    "\n",
    "        # Scale\n",
    "        qk = qk / H ** 0.5\n",
    "\n",
    "        # Mask\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device))\n",
    "        qk = qk.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Softmax\n",
    "        attention_scores = torch.softmax(qk, dim=-1)\n",
    "\n",
    "        # Dropout\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "        # MatMul (Attention scores and Value)\n",
    "        # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
    "        out = attention_scores @ v\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eafdb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(MaskedMultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(\n",
    "                embedding_dim,\n",
    "                embedding_dim // num_heads,\n",
    "                dropout\n",
    "            ) for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Scaled dot-product attention and Concat\n",
    "        # (B, T, E)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        # Linear\n",
    "        out = self.linear(out)\n",
    "\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48a0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.feed_forward_network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.feed_forward_network(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be56dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MaskedMultiHeadAttention(\n",
    "            embedding_dim,\n",
    "            num_heads,\n",
    "            dropout\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.feed_forward = FeedForward(\n",
    "            embedding_dim,\n",
    "            dropout\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Norm and Masked multi-head attention\n",
    "        out = self.multi_head_attention(self.layer_norm1(x))\n",
    "        # Add\n",
    "        x = x + out\n",
    "\n",
    "        # Norm and Feed forward\n",
    "        out = self.feed_forward(self.layer_norm2(x))\n",
    "        # Add\n",
    "        out = x + out\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58552ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.position_embedding = nn.Embedding(\n",
    "            num_embeddings=max_seq_len,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                embedding_dim,\n",
    "                num_heads,\n",
    "                dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        token_embedding = self.token_embedding(ids)\n",
    "        positional_encoding = self.position_embedding(torch.arange(0, ids.shape[-1], device=ids.device))\n",
    "        x = token_embedding + positional_encoding\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1147d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = r\"D:\\Datasets\\Tiny-Shakespeare\\All.txt\"\n",
    "SPECIAL_TOKENS = [\"<UNK>\"]\n",
    "TRAIN_SPLIT = 0.9\n",
    "EMBEDDING_DIM = 384\n",
    "NUM_HEADS = 6\n",
    "NUM_LAYERS = 6\n",
    "DROPOUT = 0.2\n",
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "SEQUENCE_LENGTH = 256\n",
    "TRAINING_ITERS = 5000\n",
    "EVAL_FREQ = 500\n",
    "EVAL_ITERS = 100\n",
    "OUTPUT_DIR = \"Models\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b2f3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATASET_PATH, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6f204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text))) + SPECIAL_TOKENS\n",
    "\n",
    "vocab_size = len(chars)\n",
    "\n",
    "char2id = {ch: i for i, ch in enumerate(chars)}\n",
    "id2char = {i: ch for ch, i in char2id.items()}\n",
    "\n",
    "string2ids = lambda s: [char2id.get(ch, char2id[\"<UNK>\"]) for ch in s]\n",
    "ids2string = lambda l: \"\".join([id2char[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0f9116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(string2ids(text), dtype=torch.int64)\n",
    "\n",
    "train_end_index = int(len(data) * TRAIN_SPLIT)\n",
    "\n",
    "train_data = data[:train_end_index]\n",
    "eval_data = data[train_end_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cb31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(\n",
    "    split: str\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    data = train_data if split == \"train\" else eval_data\n",
    "\n",
    "    # Starting index of each batch\n",
    "    start_ids = torch.randint(low=0, high=len(data) - SEQUENCE_LENGTH, size=(BATCH_SIZE,))\n",
    "\n",
    "    x = torch.stack([data[i:i + SEQUENCE_LENGTH] for i in start_ids]).to(DEVICE)\n",
    "    y = torch.stack([data[i + 1:i + SEQUENCE_LENGTH + 1] for i in start_ids]).to(DEVICE)\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12150446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(\n",
    "    model: Transformer,\n",
    "    save_dir: str = OUTPUT_DIR\n",
    ") -> None:\n",
    "    # Create directory if it doesnt exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model weights to model_weights.pth\n",
    "    torch.save(model.state_dict(), f=os.path.join(save_dir, \"model_weights.pth\"))\n",
    "\n",
    "    # Save vocabulary (character to id mapping) to vocab.pkl\n",
    "    with open(os.path.join(save_dir, \"vocab.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(char2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9aef5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(\n",
    "    model: Transformer\n",
    ") -> dict[str, float]:\n",
    "    model.eval()\n",
    "\n",
    "    out = {}\n",
    "    for split in [\"train\", \"eval\"]:\n",
    "        # Tensor to store the loss from each batch\n",
    "        losses = torch.zeros(EVAL_ITERS)\n",
    "\n",
    "        for i in range(EVAL_ITERS):\n",
    "            # Get batch of data\n",
    "            x, y = get_batch(split)\n",
    "\n",
    "            # Send to GPU\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, vocab_size),\n",
    "                y.reshape(-1)\n",
    "            )\n",
    "\n",
    "            # Store the loss\n",
    "            losses[i] = loss.item()\n",
    "\n",
    "        out[split] = losses.mean().item()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded8a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model: Transformer,\n",
    "    optimizer: torch.optim.Optimizer\n",
    ") -> None:\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_loss_step = None\n",
    "\n",
    "    for step in tqdm(range(TRAINING_ITERS)):\n",
    "        # Get batch of data\n",
    "        x, y = get_batch(split=\"train\")\n",
    "\n",
    "        # Send to GPU\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(\n",
    "            logits.reshape(-1, vocab_size),\n",
    "            y.reshape(-1)\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (step + 1) % EVAL_FREQ == 0:\n",
    "            # Evaluate the model\n",
    "            losses = evaluate(model)\n",
    "\n",
    "            print(f\"Step: {step + 1} - Training loss: {losses['train']:.4f} - Validation loss: {losses['eval']:.4f}\")\n",
    "\n",
    "            if losses[\"eval\"] < best_val_loss:\n",
    "                best_val_loss = losses[\"eval\"]\n",
    "                best_val_loss_step = step + 1\n",
    "\n",
    "            # Save model checkpoint\n",
    "            save_model(model, save_dir=os.path.join(OUTPUT_DIR, f\"checkpoint-{step + 1}\"))\n",
    "\n",
    "    if best_val_loss_step is not None:\n",
    "        model.load_state_dict(torch.load(\n",
    "            os.path.join(OUTPUT_DIR, f\"checkpoint-{best_val_loss_step}\", \"model_weights.pth\"),\n",
    "            weights_only=True\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4480b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model: Transformer,\n",
    "    prompt: str = \"\\n\",\n",
    "    max_new_tokens: int = 1000,\n",
    "    do_sample: bool = False\n",
    ") -> str:\n",
    "    assert prompt != \"\", \\\n",
    "        \"Cannot have an empty string as the prompt. Please specify a valid prompt or leave the default prompt.\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the prompt, convert to pytorch tensor, send to GPU and add batch dimension\n",
    "    ids = torch.tensor(\n",
    "        string2ids(prompt),\n",
    "        dtype=torch.int64,\n",
    "        device=DEVICE\n",
    "    ).unsqueeze(dim=0) # (B, t)\n",
    "    # Number of tokens in the prompt\n",
    "    prompt_len = ids.shape[1]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        # The model's input will be the last SEQUENCE_LENGTH tokens\n",
    "        x = ids[:, -SEQUENCE_LENGTH:] # (B, T)\n",
    "        # Get model prediction\n",
    "        logits = model(x)[:, -1, :] # (B, C)        \n",
    "        if do_sample:\n",
    "            # Get probability distribution\n",
    "            probs = torch.softmax(logits, dim=-1) # (B, C)\n",
    "            # Sample from the distribution\n",
    "            idx = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        else:\n",
    "            # Get the index with the highest probability\n",
    "            idx = torch.argmax(logits, dim=-1, keepdim=True) # (B, 1)\n",
    "        # Append sampled index to the running sequence\n",
    "        ids = torch.cat((ids, idx), dim=-1) # (B, t + 1)\n",
    "\n",
    "    # Get only the tokens generated by the model\n",
    "    response_ids = ids.squeeze(dim=0).cpu().tolist()[prompt_len:]\n",
    "    # Convert token ids to characters\n",
    "    response = ids2string(response_ids)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aaeee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab_size,\n",
    "    SEQUENCE_LENGTH,\n",
    "    EMBEDDING_DIM,\n",
    "    NUM_HEADS,\n",
    "    NUM_LAYERS,\n",
    "    DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eca80c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 4.318239688873291, 'eval': 4.3184614181518555}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf76e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 501/5000 [02:20<4:10:29,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500 - Training loss: 1.9816 - Validation loss: 2.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1001/5000 [04:41<3:43:17,  3.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000 - Training loss: 1.6146 - Validation loss: 1.7811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1501/5000 [07:01<3:16:21,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500 - Training loss: 1.4450 - Validation loss: 1.6429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2001/5000 [09:22<2:48:02,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000 - Training loss: 1.3456 - Validation loss: 1.5681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2501/5000 [11:43<2:19:58,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2500 - Training loss: 1.2778 - Validation loss: 1.5135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3001/5000 [14:04<1:51:55,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3000 - Training loss: 1.2326 - Validation loss: 1.5004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3501/5000 [16:25<1:23:56,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3500 - Training loss: 1.1866 - Validation loss: 1.4788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4001/5000 [18:46<55:53,  3.36s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4000 - Training loss: 1.1484 - Validation loss: 1.4813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4501/5000 [21:07<27:55,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4500 - Training loss: 1.1147 - Validation loss: 1.4698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [23:28<00:00,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000 - Training loss: 1.0765 - Validation loss: 1.4713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "159beb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 1.1103941202163696, 'eval': 1.472131609916687}\n"
     ]
    }
   ],
   "source": [
    "print(evaluate(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf4d25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f8e85ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are saily danger than my fellow.\n",
      "Pitch'd I am for Lady Dorset the limb,\n",
      "Which mercy you are as I set a rooman\n",
      "As Edward as Wiltshires, Pompey; and you part,\n",
      "Your brother, and friends me you so;\n",
      "If therefore, sir, no purpose, yield you hare\n",
      "Marcius fin, may be you to green to part?\n",
      "Nightful talk I do a prisoner!\n",
      "I truth Camillo, and know me?\n",
      "Therefore life to his fair queen again.\n",
      "\n",
      "BENVOLIO:\n",
      "Soft of the sword male pernive your groans.\n",
      "\n",
      "ROMEO:\n",
      "I know, my lord, I dry no more torch, sir, but all,\n",
      "that Bolingbroke's for usurping in his city,\n",
      "As guilty shall stol her beast gentlemen.\n",
      "What were man? why? In heart's in't not in the paid,\n",
      "How cut our affect with husbandful friend,\n",
      "That he shall scarce pay Tybalt discreets?\n",
      "O prayer lords, and with some make digs!\n",
      "How down, frown friend, I dress to myself.\n",
      "Ah! What thou dost away?\n",
      "\n",
      "HERMIONE:\n",
      "The catch hath the queen made be my power?\n",
      "Thou wast as I that my love? take my cousin's life,\n",
      "Read and quickrices, lodger bear thy love:\n",
      "The pembraction hath were dead a dug!\n",
      "Thou art too respect; 'twas no chequer us!\n",
      "Two this way forth, ten thou art done!\n",
      "\n",
      "KING RICHARD II:\n",
      "Stanley, do not my life had, my grief,\n",
      "But bones, and the beauty since me now,\n",
      "I entreated, I stand not to excuse thy mother\n",
      "To like the pure of you use to you: perpress,\n",
      "To make thee posters all Juliet,\n",
      "I'll play of our Roscians, Richard, where's\n",
      "Thou wouldst urge thy less bed;\n",
      "One, my edicatrators, and thou lose, 'tis prosperous blood thee\n",
      "To complain; so confession as it were any\n",
      "Deserved of accourt of the darks.\n",
      "\n",
      "ROMEO:\n",
      "I thank my pace. I must value to my worth.\n",
      "\n",
      "BENVOLIO:\n",
      "Pardon me there was: alas, I will give my reign\n",
      "\n",
      "First Murderer:\n",
      "I would find the Burgundy is and not forth\n",
      "Such dismasterity as thee assured my sons\n",
      "Well see could scrow us my soul.\n",
      "\n",
      "BENVOLIO:\n",
      "And then she lineve should know no more.\n",
      "What I, think'st thou art another take order:\n",
      "Go, Montague, and is question'd!\n",
      "\n",
      "MERCUTIO:\n",
      "O, strike other, being a woman, when strong to\n",
      "me o'er-special: yet souls, as stages come to your tent.'\n",
      "\n",
      "BENVOLIO:\n",
      "Thus, I weight, I would be jest.\n",
      "\n",
      "ROMEO:\n",
      "Gracious prince! hath have he done?\n",
      "\n",
      "ROMEO:\n",
      "My lords must and shamed in goodness of hoste,\n",
      "Dissemble neglection! I am you communed;\n",
      "And she wounds me that can make me a solemn world.\n",
      "Where is the nurse of thy joy?\n",
      "Tell him that I must have apped, I do meet you say!\n",
      "Yea, be thou wilt not lean not to recough.\n",
      "\n",
      "LADY CAPULET:\n",
      "Then make the bed of your grace:\n",
      "And, my lord, so love me to a love,\n",
      "As if you pass you must speak up in the world:\n",
      "A happy devoughs and thou welcome in the swrit lady,\n",
      "Our emdering of the officers' looks.\n",
      "\n",
      "LADY CAPULET:\n",
      "He should be outward of this I am desert,\n",
      "Since the purpose.\n",
      "\n",
      "Second Murderer:\n",
      "O part, love, as you on me as long your graces to\n",
      "The pleasure queen.\n",
      "\n",
      "Nurse:\n",
      "Come, sir. This is great words?\n",
      "Gave us take before-born time, stay me.\n",
      "I was in the pleasure of a high battle year:\n",
      "And to you unknown a parlous of his which duke\n",
      "When he did again; I stand whom your eyes,\n",
      "To an instilly mortal find, look on valiant\n",
      "Recoves up, and have of more than to jealous\n",
      "Than the noble childs of the harlosom of sother's buried,\n",
      "That beast corse should doing: but as the reign way\n",
      "To itself can of you fall tear to me rather\n",
      "Must no so remainined with Rutland than a man it.\n",
      "\n",
      "CLARENCE:\n",
      "I am not yet he will.\n",
      "But Harry doth not look the carket's face:\n",
      "And if you passes grown in you and at you\n",
      "what begs as that abserved? O gentle more!\n",
      "\n",
      "SLY:\n",
      "So impossible a prisoner!\n",
      "My shape is grave!\n",
      "\n",
      "CLARENCE:\n",
      "My lord; but I, Marcius! What, hope\n",
      "That that Lady Aufidius? my dear is dead!\n",
      "\n",
      "GLOUCESTER:\n",
      "No more, my lord, such as is the people in the house\n",
      "As if the woes as every bear as\n",
      "Was the cause of kind, where is my both of will.\n",
      "\n",
      "LADY ANNE:\n",
      "My lord, if these words are of these done,\n",
      "How landed in all my sweet wanders laughs,\n",
      "That if I cannot, fall a doing slaughter'd his own,\n",
      "Therefore lie, or do was my true better Juliet\n",
      "As my true than as as my nature wouldst to\n",
      "And do make the lip to make off an weaking-faced blood\n",
      "When he shall seek against my quoth king: met the\n",
      "Will be pilgrim'd in brook of my brother:\n",
      "When Clarence, fair lord--\n",
      "\n",
      "BUCKINGHAM:\n",
      "What! then, by Alas! noble before 'us, Barnardina,\n",
      "Not a poison of my judge?\n",
      "\n",
      "Gentleman:\n",
      "Then, it pleased your people,\n",
      "All ten times to rank as your victory,\n",
      "We promised. For mother, love gift no bed,\n",
      "That laugh you do hate, you never to him.\n",
      "What will? an England now will discover,\n",
      "And tell your justure from hence and Sicilia.\n",
      "\n",
      "ANGELO:\n",
      "Sweet Woodmas?\n",
      "\n",
      "CAMILLO:\n",
      "Now news, which away with his little part.\n",
      "\n",
      "ISABELLA:\n",
      "What, my lord?\n",
      "\n",
      "CAPULET:\n",
      "Will you lie to do I fear?\n",
      "What's no succession? Come of comfort their\n",
      "enternal straight would have went on a cruel wooing?\n",
      "A slew'st divided struck'd two make him more.\n",
      "What, most one name waking; but so here\n",
      "That, are it stay; we beseech, from your love\n",
      "And natural.\n",
      "\n",
      "CLIFFORD:\n",
      "Tell thee, of Aufidius Marcius,\n",
      "You say all to London that wayward you at once,\n",
      "Leave to the happy haste, to churchy\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, max_new_tokens=5000, do_sample=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c7313c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No; good Montagues; I for thee have Edward in thy service,\n",
      "By God's, unwilling name to be particular,\n",
      "Or what they seems for roaring peace doth between\n",
      "To let us upon the wrongful tire wars,\n",
      "That stirrings in any high of circumsal eye,\n",
      "Which hare in his dafferent royalty\n",
      "Such treason with that middle envious sword:\n",
      "Hark you told him, in his strength.\n",
      "\n",
      "POLIXENES:\n",
      "Ay, if the duty's approach.\n",
      "But the tidil bend of his princesss\n",
      "Richmond and succession of pipe itself,\n",
      "That drums did not till still with his place\n",
      "That usurprishes be drink when the numbers\n",
      "Which air effections brooks up as joint\n",
      "The old which storm way or your counsel?\n",
      "\n",
      "JULIET:\n",
      "There is a pair that you have snow.\n",
      "I'll brought you go along. What Cominius is most he,\n",
      "Thou pass'd my heart, your favours will be gone?\n",
      "\n",
      "ROMEO:\n",
      "I will convise in thy people?\n",
      "O, breathe your world's resolute envious.\n",
      "Edward, desert thou welcome?\n",
      "\n",
      "AEdile:\n",
      "O, part, but not that I will stand:\n",
      "The horse: at proffendon serves no inherappy sun\n",
      "So the king\n"
     ]
    }
   ],
   "source": [
    "print(generate(\n",
    "    model,\n",
    "    prompt=\"Enter two rivals at dawn.\\n\\nFirst Rival:\",\n",
    "    do_sample=True\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
