{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a3ca1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "035a83e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        head_dim: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(AttentionHead, self).__init__()\n",
    "\n",
    "        self.query = nn.Linear(embedding_dim, head_dim)\n",
    "        self.key = nn.Linear(embedding_dim, head_dim)\n",
    "        self.value = nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linear\n",
    "        # (B, T, E) -> (B, T, H)\n",
    "        q = self.query(x)\n",
    "        # (B, T, E) -> (B, T, H)\n",
    "        k = self.key(x)\n",
    "        # (B, T, E) -> (B, T, H)\n",
    "        v = self.value(x)\n",
    "\n",
    "        _, T, H = k.shape\n",
    "\n",
    "        # MatMul (Query and Transpose of Key)\n",
    "        # (B, T, H) @ (B, H, T) -> (B, T, T)\n",
    "        qk = q @ k.transpose(-1, -2)\n",
    "\n",
    "        # Scale\n",
    "        qk = qk / H ** 0.5\n",
    "\n",
    "        # Mask\n",
    "        mask = torch.tril(torch.ones(T, T, device=x.device))\n",
    "        qk = qk.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "        # Softmax\n",
    "        attention_scores = torch.softmax(qk, dim=-1)\n",
    "\n",
    "        # Dropout\n",
    "        attention_scores = self.dropout(attention_scores)\n",
    "\n",
    "        # MatMul (Attention scores and Value)\n",
    "        # (B, T, T) @ (B, T, H) -> (B, T, H)\n",
    "        out = attention_scores @ v\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eafdb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(MaskedMultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            AttentionHead(\n",
    "                embedding_dim,\n",
    "                embedding_dim // num_heads,\n",
    "                dropout\n",
    "            ) for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        self.linear = nn.Linear(embedding_dim, embedding_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Scaled dot-product attention and Concat\n",
    "        # (B, T, E)\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "\n",
    "        # Linear\n",
    "        out = self.linear(out)\n",
    "\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d48a0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.feed_forward_network = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embedding_dim * 4, embedding_dim)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.feed_forward_network(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be56dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MaskedMultiHeadAttention(\n",
    "            embedding_dim,\n",
    "            num_heads,\n",
    "            dropout\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        self.feed_forward = FeedForward(\n",
    "            embedding_dim,\n",
    "            dropout\n",
    "        )\n",
    "        self.layer_norm2 = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Norm and Masked multi-head attention\n",
    "        out = self.multi_head_attention(self.layer_norm1(x))\n",
    "        # Add\n",
    "        x = x + out\n",
    "\n",
    "        # Norm and Feed forward\n",
    "        out = self.feed_forward(self.layer_norm2(x))\n",
    "        # Add\n",
    "        out = x + out\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58552ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        num_layers: int,\n",
    "        dropout: float\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.position_embedding = nn.Embedding(\n",
    "            num_embeddings=max_seq_len,\n",
    "            embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                embedding_dim,\n",
    "                num_heads,\n",
    "                dropout\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        token_embedding = self.token_embedding(ids)\n",
    "        positional_encoding = self.position_embedding(torch.arange(0, ids.shape[-1], device=ids.device))\n",
    "        x = token_embedding + positional_encoding\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25228c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareGPT:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int = 256,\n",
    "        batch_size: int = 64,\n",
    "        embedding_dim: int = 384,\n",
    "        num_heads: int = 6,\n",
    "        num_layers: int = 6,\n",
    "        dropout: float = 0.2,\n",
    "        optimizer_cls: type[torch.optim.Optimizer] = torch.optim.AdamW,\n",
    "        learning_rate: float = 3e-4,\n",
    "        optimizer_kwargs: dict = {},\n",
    "        output_dir: str | None = \"results\",\n",
    "        device: str = \"auto\"\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer_cls = optimizer_cls\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "        self.output_dir = output_dir\n",
    "        self.device = torch.device(\"cuda\" if device != \"cpu\" and torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.model = Transformer(\n",
    "            vocab_size,\n",
    "            max_seq_len,\n",
    "            embedding_dim,\n",
    "            num_heads,\n",
    "            num_layers,\n",
    "            dropout\n",
    "        ).to(self.device)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_data: torch.Tensor,\n",
    "        eval_data: torch.Tensor,\n",
    "        train_steps: int = 5000,\n",
    "        eval_freq: int = 500,\n",
    "        load_best_model_at_end: bool = False\n",
    "    ) -> None:\n",
    "        # If it is required to load the best model at the end, ensure output_dir is specified to save model checkpoints\n",
    "        if load_best_model_at_end:\n",
    "            assert self.output_dir is not None, \\\n",
    "                \"output_dir must be specified to save model checkpoints in order to load the best model at the end of training.\"\n",
    "            \n",
    "        # Send data to GPU\n",
    "        train_data = train_data.to(self.device)\n",
    "        eval_data = eval_data.to(self.device)\n",
    "\n",
    "        optimizer = self.optimizer_cls(\n",
    "            self.model.parameters(),\n",
    "            lr=self.learning_rate,\n",
    "            **self.optimizer_kwargs\n",
    "        )\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_val_loss_step = None\n",
    "\n",
    "        for step in tqdm(range(train_steps)):\n",
    "            # Get batch of data\n",
    "            x, y = self._get_batch(train_data)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = self.model(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, self.vocab_size),\n",
    "                y.reshape(-1)\n",
    "            )\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % eval_freq == 0:\n",
    "                # Evaluate the model\n",
    "                train_loss = self.evaluate(train_data)\n",
    "                val_loss = self.evaluate(eval_data)\n",
    "                print(f\"Step: {step + 1} - Training loss: {train_loss:.4f} - Validation loss: {val_loss:.4f}\")\n",
    "\n",
    "                if self.output_dir is not None:\n",
    "                    # Save model checkpoint\n",
    "                    self.save(save_path=os.path.join(self.output_dir, f\"checkpoint-{step + 1}\", \"model_weights.pth\"))\n",
    "\n",
    "                    if val_loss < best_val_loss:\n",
    "                        best_val_loss = val_loss\n",
    "                        best_val_loss_step = step + 1\n",
    "\n",
    "        if load_best_model_at_end and best_val_loss_step is not None:\n",
    "            self.model.load_state_dict(torch.load(\n",
    "                os.path.join(self.output_dir, f\"checkpoint-{best_val_loss_step}\", \"model_weights.pth\"),\n",
    "                weights_only=True\n",
    "            ))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(\n",
    "        self,\n",
    "        data: torch.Tensor\n",
    "    ) -> float:\n",
    "        # Send data to GPU\n",
    "        data = data.to(self.device)\n",
    "        \n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        # Tensor to store the loss from each batch\n",
    "        losses = torch.zeros(100)\n",
    "\n",
    "        for i in range(100):\n",
    "            # Get batch of data\n",
    "            x, y = self._get_batch(data)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = self.model(x)\n",
    "            loss = F.cross_entropy(\n",
    "                logits.reshape(-1, self.vocab_size),\n",
    "                y.reshape(-1)\n",
    "            )\n",
    "\n",
    "            # Store the loss from the batch\n",
    "            losses[i] = loss.item()\n",
    "\n",
    "        # Calculate mean loss\n",
    "        loss = losses.mean().item()\n",
    "\n",
    "        # Set model back to training mode\n",
    "        self.model.train()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        max_new_tokens: int = 1000,\n",
    "        do_sample: bool = False\n",
    "    ) -> torch.Tensor:\n",
    "        # input_ids can be either 1D (seq_len,) or 2D (batch_size, seq_len)\n",
    "        assert input_ids.ndim in (1, 2), \\\n",
    "            \"input_ids must be a 1D or 2D tensor.\"\n",
    "        # If 1D, ensure there is at least 1 element and add batch dimension\n",
    "        if input_ids.ndim == 1:\n",
    "            assert input_ids.shape[0] > 0, \\\n",
    "                \"input_ids must contain at least 1 element.\"\n",
    "            input_ids = input_ids.unsqueeze(dim=0)\n",
    "        # If 2D, ensure both dimensions are greater than 0\n",
    "        else:\n",
    "            assert input_ids.shape[0] > 0, \\\n",
    "                \"Batch size (dim=0) must be greater than 0.\"\n",
    "            assert input_ids.shape[1] > 0, \\\n",
    "                \"Sequence length (dim=1) must be greater than 0.\"\n",
    "        # Send to GPU\n",
    "        input_ids = input_ids.to(self.device) # (B, t)\n",
    "\n",
    "        # Set model to evaluation mode\n",
    "        self.model.eval()\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            # The model's input will be the last max_seq_len tokens of each sequence\n",
    "            x = input_ids[:, -self.max_seq_len:] # (B, T)\n",
    "            # Get model predictions\n",
    "            logits = self.model(x)[:, -1, :] # (B, C)\n",
    "            if do_sample:\n",
    "                # Get probability distributions\n",
    "                probs = torch.softmax(logits, dim=-1) # (B, C)\n",
    "                # Sample from the distributions\n",
    "                ids = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            else:\n",
    "                # Get the indexes with the highest probabilities\n",
    "                ids = torch.argmax(logits, dim=-1, keepdim=True) # (B, 1)\n",
    "            # Append sampled indexes to the running sequences\n",
    "            input_ids = torch.cat((input_ids, ids), dim=-1) # (B, t + 1)\n",
    "\n",
    "        # Set model back to training mode\n",
    "        self.model.train()\n",
    "        \n",
    "        return input_ids.cpu()\n",
    "    \n",
    "    def save(\n",
    "        self,\n",
    "        save_path: str | None = None\n",
    "    ) -> None:\n",
    "        if save_path is None:\n",
    "            # If save_path is not provided, derive from output_dir\n",
    "            assert self.output_dir is not None, \\\n",
    "                \"As output_dir is not specified, you must provide a valid save_path to save the model.\"\n",
    "            save_path = os.path.join(self.output_dir, \"model_weights.pth\")\n",
    "\n",
    "        # Create save directory if it doesnt exist\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        \n",
    "        # Save model weights\n",
    "        torch.save(self.model.state_dict(), f=save_path)\n",
    "\n",
    "    def load_model(\n",
    "        self,\n",
    "        save_path: str | None = None\n",
    "    ) -> None:\n",
    "        if save_path is None:\n",
    "            # If save_path is not provided, derive from output_dir\n",
    "            assert self.output_dir is not None, \\\n",
    "                \"As output_dir is not specified, you must provide a valid save_path to load the tokenizer.\"            \n",
    "            save_path = os.path.join(self.output_dir, \"model_weights.pth\")\n",
    "\n",
    "        # Ensure save_path is valid\n",
    "        assert os.path.exists(save_path), \\\n",
    "            f\"save_path '{save_path}' does not exist. Please provide a valid path.\"\n",
    "        \n",
    "        # Load model weights\n",
    "        self.model.load_state_dict(torch.load(save_path, map_location=self.device))\n",
    "\n",
    "    def _get_batch(\n",
    "        self,\n",
    "        data: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Starting index of each batch\n",
    "        start_ids = torch.randint(\n",
    "            low=0,\n",
    "            high=len(data) - self.max_seq_len,\n",
    "            size=(self.batch_size,)\n",
    "        )\n",
    "\n",
    "        # Input sequences (max_seq_len tokens starting from each of the start_ids)\n",
    "        x = torch.stack(\n",
    "            [data[i:i + self.max_seq_len] for i in start_ids]\n",
    "        ).to(self.device)\n",
    "        # Target sequences (input sequences shifted right by one token)\n",
    "        y = torch.stack(\n",
    "            [data[i + 1:i + self.max_seq_len + 1] for i in start_ids]\n",
    "        ).to(self.device)\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41c6e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterLevelTokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        special_tokens: list[str] = [],\n",
    "        output_dir: str | None = \"results\"\n",
    "    ):\n",
    "        self.special_tokens = special_tokens\n",
    "        self.vocab_size = len(special_tokens)\n",
    "        self.char2idx = {c: i for i, c in enumerate(special_tokens)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "\n",
    "        self.output_dir = output_dir\n",
    "        if output_dir is not None:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        text: str\n",
    "    ) -> None:\n",
    "        # Get all unique characters in the text\n",
    "        chars = sorted(list(set(text)))\n",
    "\n",
    "        # Add all unique characters to the vocabulary\n",
    "        self.vocab_size = len(self.special_tokens) + len(chars)\n",
    "        self.char2idx = {c: i for i, c in enumerate(self.special_tokens + chars)}\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        text: str,\n",
    "        return_tensor: bool = False\n",
    "    ) -> list[int] | torch.Tensor:\n",
    "        ids = []\n",
    "\n",
    "        for char in text:\n",
    "            if char in self.char2idx:\n",
    "                # Append corresponding ID to the list if the character is in the vocabulary\n",
    "                ids.append(self.char2idx[char])\n",
    "            else:\n",
    "                # Skip the character if it is not in the vocabulary\n",
    "                print(f\"Warning: Skipping the character '{char}' as it is not in the vocabulary.\")\n",
    "        \n",
    "        # Convert to tensor if required\n",
    "        if return_tensor:\n",
    "            return torch.tensor(ids, dtype=torch.int64)\n",
    "        \n",
    "        return ids\n",
    "    \n",
    "    def decode(\n",
    "        self,\n",
    "        ids: list[int] | torch.Tensor\n",
    "    ) -> str:\n",
    "        # Convert ids to a list if it is a tensor\n",
    "        if isinstance(ids, torch.Tensor):\n",
    "            ids = ids.tolist()\n",
    "\n",
    "        string = \"\"\n",
    "\n",
    "        for idx in ids:\n",
    "            # Skip special tokens\n",
    "            if idx >= len(self.special_tokens):\n",
    "                if idx in self.idx2char:\n",
    "                    # Append corresponding character to the string if the ID is in the vocabulary\n",
    "                    string += self.idx2char[idx]\n",
    "                else:\n",
    "                    # Skip if the ID is not in the vocabulary\n",
    "                    print(f\"Warning: Skipping the ID {idx} as it is not in the vocabulary.\")\n",
    "        \n",
    "        return string\n",
    "    \n",
    "    def save(\n",
    "        self,\n",
    "        save_path: str | None = None\n",
    "    ) -> None:\n",
    "        if save_path is None:\n",
    "            # If save_path is not provided, derive from output_dir\n",
    "            assert self.output_dir is not None, \\\n",
    "                \"As output_dir is not specified, you must provide a valid save_path to save the tokenizer.\"            \n",
    "            save_path = os.path.join(self.output_dir, \"tokenizer.pkl\")\n",
    "\n",
    "        # Create save directory if it doesnt exist\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        # Save tokenizer state to a pickle file\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            pickle.dump({\n",
    "                \"special_tokens\": self.special_tokens,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"char2idx\": self.char2idx,\n",
    "                \"idx2char\": self.idx2char\n",
    "            }, f)\n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        save_path: str | None = None\n",
    "    ) -> None:\n",
    "        if save_path is None:\n",
    "            # If save_path is not provided, derive from output_dir\n",
    "            assert self.output_dir is not None, \\\n",
    "                \"As output_dir is not specified, you must provide a valid save_path to load the tokenizer.\"            \n",
    "            save_path = os.path.join(self.output_dir, \"tokenizer.pkl\")\n",
    "\n",
    "        # Ensure save_path is valid\n",
    "        assert os.path.exists(save_path), \\\n",
    "            f\"save_path '{save_path}' does not exist. Please provide a valid path.\"\n",
    "\n",
    "        with open(save_path, \"rb\") as f:\n",
    "            # Load tokenizer state from a pickle file\n",
    "            data = pickle.load(f)\n",
    "\n",
    "            # Set tokenizer attributes\n",
    "            self.special_tokens = data[\"special_tokens\"]\n",
    "            self.vocab_size = data[\"vocab_size\"]\n",
    "            self.char2idx = data[\"char2idx\"]\n",
    "            self.idx2char = data[\"idx2char\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b2f3c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\n",
    "    r\"D:\\Datasets\\Tiny-Shakespeare\\All.txt\",\n",
    "    \"r\",\n",
    "    encoding=\"utf-8\",\n",
    "    errors=\"replace\"\n",
    ") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eea05e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharacterLevelTokenizer(\n",
    "    special_tokens=[\"<UNK>\"],\n",
    "    output_dir=\"results\"\n",
    ")\n",
    "tokenizer.train(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80d0871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShakespeareGPT(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    max_seq_len=256,\n",
    "    batch_size=64,\n",
    "    embedding_dim=384,\n",
    "    num_heads=6,\n",
    "    num_layers=6,\n",
    "    dropout=0.2,\n",
    "    optimizer_cls=torch.optim.AdamW,\n",
    "    learning_rate=3e-4,\n",
    "    output_dir=\"results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0f9116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenizer.encode(text, return_tensor=True)\n",
    "\n",
    "train_end_index = int(len(data) * 0.9)\n",
    "\n",
    "train_data = data[:train_end_index]\n",
    "eval_data = data[train_end_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07cdd1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.375035762786865\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf76e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 501/5000 [02:19<4:08:49,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 500 - Training loss: 1.9806 - Validation loss: 2.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1001/5000 [04:39<3:41:13,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000 - Training loss: 1.6008 - Validation loss: 1.7737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 1501/5000 [06:58<3:14:00,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1500 - Training loss: 1.4423 - Validation loss: 1.6476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2001/5000 [09:18<2:46:22,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2000 - Training loss: 1.3432 - Validation loss: 1.5751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 2501/5000 [11:38<2:18:43,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 2500 - Training loss: 1.2798 - Validation loss: 1.5273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3001/5000 [13:58<1:50:40,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3000 - Training loss: 1.2267 - Validation loss: 1.4892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 3501/5000 [16:17<1:23:06,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 3500 - Training loss: 1.1800 - Validation loss: 1.4803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4001/5000 [18:37<55:23,  3.33s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4000 - Training loss: 1.1440 - Validation loss: 1.4767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4501/5000 [20:57<27:42,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 4500 - Training loss: 1.1060 - Validation loss: 1.4683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [23:17<00:00,  3.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000 - Training loss: 1.0736 - Validation loss: 1.4723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    train_data,\n",
    "    eval_data,\n",
    "    train_steps=5000,\n",
    "    eval_freq=500,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "159beb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.468522071838379\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(eval_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf4d25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save()\n",
    "tokenizer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f8e85ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Where three, if this, they were to God's tent.\n",
      "\n",
      "First Gentleman:\n",
      "'Honest man, be perhaps, and not to't.\n",
      "Why, sir is my own replies; there's throw I am love,\n",
      "But they have deserved with him marriage\n",
      "And look your own ears for pardon but royalty.\n",
      "This tender seems pretised with our foes,\n",
      "Envire Angelo, in good fellows and beague.\n",
      "To be call'd fearful a fellow tomorrow.\n",
      "\n",
      "RATCLIFF:\n",
      "I say, kind what a horse! Why fool!\n",
      "\n",
      "KING RICHARD II:\n",
      "Ay, I am content to the way?\n",
      "\n",
      "RATCLIFF:\n",
      "\n",
      "KING RICHARD III:\n",
      "Do, sweet me, desperate, of you must die,\n",
      "That makes no gentleman weight me, never are yet to you;\n",
      "And farewell long by this hourd in my law\n",
      "Not death that blest you and still you.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Ay, so your brother's member: a gate of wonder,\n",
      "To put you take an one about to proclaim.\n",
      "\n",
      "YORK:\n",
      "You may fought of the prayers may wive your hands;\n",
      "And three better than ever you should not\n",
      "Confirm proved warwick.\n",
      "\n",
      "DY CAPULET:\n",
      "What off?\n",
      "\n",
      "Nurse:\n",
      "You pray to be not here?\n",
      "She was reverend our honour in a gentleman:\n",
      "Belike messenger the laws, but that, I hope,\n",
      "But hie you conmends to you; very being pack by there.\n",
      "\n",
      "SICINIUS:\n",
      "He was so? or how they are picts gone,\n",
      "And she are as they are pitching come good.\n",
      "\n",
      "BRUTUS:\n",
      "Why, no, the last,\n",
      "And that call'd hopes to my stronger.\n",
      "\n",
      "MENENIUS:\n",
      "Let's here;\n",
      "If he must resign enters to earnest.\n",
      "\n",
      "BRUTUS:\n",
      "'Sirrah on their country, he hath provided tell'd\n",
      "Come hither, and you pleased him overboard,\n",
      "In mine eyes importion.\n",
      "\n",
      "MENENIUS:\n",
      "Go ye, no;\n",
      "There good Masters, the cause of the nurse.\n",
      "\n",
      "AEdile:\n",
      "We will speak against the heart for come all.\n",
      "Ark you, sir; you shall not make robber these sad:\n",
      "Give me we this exile me good Montague,\n",
      "Since with the clamour of your last were say your coat;\n",
      "The duke since so the faults and frowns of mine,\n",
      "Are they say, true but and put to a mind-chanced;\n",
      "And let them forsake like a lawful sport:\n",
      "They, though I know not turn thy hereafter.\n",
      "Nay, poor man, in pits distorm basely dismall,\n",
      "I had fortune to the mockery of the sword.\n",
      "\n",
      "PARIS:\n",
      "Go, fellow, sir.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "To you towards so. Thus we do; if then in gentleman in you\n",
      "Consul, we'll have heard--\n",
      "Go not prove me to what's the body tower,\n",
      "To wait, Paul, cheaven, if you play all the best.\n",
      "You this wise rush and fearful hand!\n",
      "What were I loved not a best? When I may be well?\n",
      "Have I been a tyrant farewell-simpledged in my blimishes\n",
      "He called forth a middle by the oracle\n",
      "With a freshes of him, own on the woe in were,\n",
      "Or never be your jecture must very to down,\n",
      "Were not my body heart in person.\n",
      "\n",
      "ROMEO:\n",
      "Take either betweens my word,\n",
      "And walk, that this remedy hand oe all,\n",
      "Their heads ere to tear it again.\n",
      "\n",
      "Second Murderer:\n",
      "\n",
      "CLARENCE:\n",
      "Why, what of the city? boy, having we stabbed with despite\n",
      "Of that tune?\n",
      "\n",
      "First Lord:\n",
      "Will't better be not what speak traitor,\n",
      "And say 'I will greet the breath?\n",
      "\n",
      "CLARENCE:\n",
      "O, what perceivest thou didst doth speak,\n",
      "Whereby they are full of conducted victory;\n",
      "With their authority shouldings to Henry.\n",
      "\n",
      "LADY CAPULET:\n",
      "Welcome, my lord.\n",
      "\n",
      "PARIS:\n",
      "So tell thou told me to the Francis, and idle him,\n",
      "How shall I may move him too lap attending throat.\n",
      "\n",
      "ANGELO:\n",
      "If it doth you forfeit be twenty sociated\n",
      "To one but your willing-dancing and braze.\n",
      "\n",
      "LADY CAPULET:\n",
      "O, what doth this is your eyess is no honour\n",
      "Your father.\n",
      "\n",
      "LEONTES:\n",
      "Why, dost thou humble this; there is best\n",
      "Give what his is well the next from my blocks.\n",
      "\n",
      "MARCIUS:\n",
      "He hath a plagued time away, he shall respected\n",
      "As if this dayling such truth most scause,\n",
      "Is near as the thrusty of the way\n",
      "Some from me, if thy bildness hath been so.\n",
      "Nay, as I hear thou break stones from me deposed.\n",
      "\n",
      "MARCIUS:\n",
      "O night be so. But do now, sir, Ispected,\n",
      "The gods and made thee to repeal ne'er sure.\n",
      "\n",
      "MARCIUS:\n",
      "Like an our noble,\n",
      "For one forenation of your grace is true.\n",
      "\n",
      "VOLUMNIA:\n",
      "A treacherous sin! He was fearful matter\n",
      "The gods of your rancour?\n",
      "\n",
      "MAMILLIUS:\n",
      "The possession of true, take with me thy foe.\n",
      "\n",
      "CORIOLANUS:\n",
      "The flier past.\n",
      "Farewell: then, get you serve me heard\n",
      "God take henry.\n",
      "\n",
      "DUCHESS:\n",
      "God fear me, let poor go.\n",
      "\n",
      "CAMILLO:\n",
      "Then, sir. Paulina care!\n",
      "\n",
      "DORCAS:\n",
      "No, I'll not like your silent liege.\n",
      "\n",
      "SICINIUS:\n",
      "I dread you'll pardon my true:\n",
      "For I fear ourself,\n",
      "Or, I said, but sleep myself, and I under you!\n",
      "If you must poison, and do your majesty,\n",
      "And she be\n",
      "Doth a self, as I am. Why, sir, let my prayer,\n",
      "That hath been borne straight in the insaint\n",
      "Of our place and weak of the hoped with old\n",
      "Kindred six ointed victory, did set on mine: so,\n",
      "The greater is well the twenty of you\n",
      "To execute vassal and by other twains\n",
      "The midress of Bohemia and the way had not a bond.\n",
      "\n",
      "KING RICHARD III:\n",
      "Hear hhap, nurse; even now, Father, speak!\n",
      "\n",
      "ARCHIDAMUS:\n",
      "He rejoice\n",
      "In peace, but eagles, what had said here alike it?\n",
      "\n",
      "CAMILLO:\n",
      "Go, save you not not behold it. I have respected not\n",
      "My pair is Anside smoke a hope:\n",
      "I do beseech you as hope as it. Fear me, I say;\n",
      "he you told you home the moan as physic:\n",
      "Long his sister have a husband and alliance;\n",
      "Or, if I am noble to do well myself, to be straight\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\\n\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensor=True)\n",
    "generated_ids = model.generate(input_ids, max_new_tokens=5000, do_sample=True)\n",
    "generated = tokenizer.decode(generated_ids[0])\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c7313c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter two rivals at dawn.\n",
      "\n",
      "First Rival:\n",
      "Then can I love to Braint, his grace as Paulina blood\n",
      "Have they made for the royal that will begin too.\n",
      "\n",
      "Third Citizen:\n",
      "Beseech you, my lord?\n",
      "\n",
      "TYRREL:\n",
      "And heart your highness so ripe is no man's.\n",
      "Patiently, and then: even you have leave the heart the law,\n",
      "Even all now you would grow our boy, we pray,\n",
      "The tribune, and this oient will seek up you.\n",
      "\n",
      "First Murderer:\n",
      "What is the kind he would be could as lie,\n",
      "And there's heaven mean, being kingdom with him.\n",
      "\n",
      "CLARENCE:\n",
      "How soft with his amendam?\n",
      "\n",
      "CLARENCE:\n",
      "What's the news? and who callsh wither?\n",
      "\n",
      "AMILLIUS:\n",
      "Which wants the right of time?\n",
      "\n",
      "HASTINGS:\n",
      "And gives one here in Christon.\n",
      "\n",
      "YORK:\n",
      "Music is true service and Nessful and poison;\n",
      "For every old man she may sprow to ourselves.\n",
      "\n",
      "SICINIUS:\n",
      "For this matter, peace!\n",
      "\n",
      "CORIOLANUS:\n",
      "If it be not apprehended: come, sir.\n",
      "\n",
      "First Gentleman:\n",
      "For the lark of Margaret and Clarence\n",
      "Tybalt, Isabel; a Montague many\n",
      "That whose fellows inferior tears have wround fought\n",
      "By what she is double in who then he prison\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Enter two rivals at dawn.\\n\\nFirst Rival:\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, return_tensor=True)\n",
    "generated_ids = model.generate(input_ids, do_sample=True)\n",
    "generated = tokenizer.decode(generated_ids[0])\n",
    "print(generated)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
